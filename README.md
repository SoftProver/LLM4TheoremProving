## Surveys

1. **A Survey on Deep Learning for Theorem Proving** `COLM 2024` [[paper]](https://arxiv.org/pdf/2404.09939)

   *Zhaoyu Li, Jialiang Sun, Logan Murphy, Qidong Su, Zenan Li, Xian Zhang, Kaiyu Yang, and Xujie Si* 



## Datasets and Benchmark
1. **Lean Workbook: A large-scale Lean problem set formalized from natural language math problems** `NeurIPS 2025 dataset and benchmark track` [[paper]](https://arxiv.org/abs/2406.03847)
  
   *Huaiyuan Ying, Zijian Wu, Yihan Geng, Jiayu Wang, Dahua Lin, Kai Chen*

## Data Synthesis
1. **STP: Self-play LLM Theorem Provers with Iterative Conjecturing and Proving**
`ICML 2025` [[paper]](https://arxiv.org/abs/2502.00212)

   *Kefan Dong, Tengyu Ma*


## Hammer-like ATPs
1. **Lean-SMT: An SMT tactic for discharging proof goals in Lean** `CAV 2025` [[paper]](https://arxiv.org/abs/2505.15796)

   *Abdalrhman Mohamed, Tomaz Mascarenhas, Harun Khan, Haniel Barbosa, Andrew Reynolds, Yicheng Qian, Cesare Tinelli, and Clark Barrett*

2. **Lean-auto: An Interface between Lean 4 and Automated Theorem Provers** `CAV 2025` [[paper]](https://arxiv.org/abs/2505.14929)

   *Yicheng Qian, Joshua Clune, Clark Barrett, Jeremy Avigad*

## Draft, Sketch, Prove (DSP)-like ATPs

1. **Draft, Sketch, and Prove: Guiding Formal Theorem Provers with Informal Proofs** `ICLR 2023` [[paper]](https://arxiv.org/abs/2210.12283)

   *Albert Q. Jiang, Sean Welleck, Jin Peng Zhou, Wenda Li, Jiacheng Liu, Mateja Jamnik, Timothée Lacroix, Yuhuai Wu, Guillaume Lample*


2. **Efficient Neural Theorem Proving via Fine-grained Proof Structure Analysis** `ICML 2025` [[paper]](https://arxiv.org/abs/2501.18310)

   *Haoxiong Liu, Jiacheng Sun, Zhenguo Li, Andrew C Yao*

## Large language model (LLM)-based ATPs

1. **Baldur: Whole-Proof Generation and Repair with Large Language Models** `ESEC/FSE 2023` [[paper]](https://arxiv.org/abs/2303.04910)

   *Emily First, Markus N. Rabe, Talia Ringer, Yuriy Brun*


## Autoformalization

1. TBD

## Specification

1. **nl2spec: Interactively Translating Unstructured Natural Language to Temporal Logics with Large Language Models** `CAV 2023` [[paper]](https://arxiv.org/abs/2303.04864)

   *Matthias Cosler, Christopher Hahn, Daniel Mendoza, Frederik Schmitt, Caroline Trippel*

2. **Enchanting Program Specification Synthesis by Large Language Models using Static Analysis and Program Verification** `CAV 2024` [[paper]](https://arxiv.org/abs/2404.00762)

   *Cheng Wen, Jialun Cao, Jie Su, Zhiwu Xu, Shengchao Qin, Mengda He, Haokun Li, Shing-Chi Cheung, Cong Tian*

## Premise Selection

1. **LeanDojo: Theorem Proving with Retrieval-Augmented Language Models** `NeurIPS 2023 dataset and benchmark track` [[paper]](https://arxiv.org/abs/2306.15626)

   *Kaiyu Yang, Aidan M. Swope, Alex Gu, Rahul Chalamala, Peiyang Song, Shixing Yu, Saad Godil, Ryan Prenger, Anima Anandkumar*

2. **DeepMath - Deep Sequence Models for Premise Selection** `NeurIPS 2016` [[paper]](https://arxiv.org/abs/1606.04442)

   *Alex A. Alemi, Francois Chollet, Niklas Een, Geoffrey Irving, Christian Szegedy, Josef Urban*

3. **HolStep: A Machine Learning Dataset for Higher-order Logic Theorem Proving** `ICLR 2017` [[paper]](https://arxiv.org/abs/1703.00426)

   *Cezary Kaliszyk, François Chollet, Christian Szegedy*

4. **Premise Selection for Theorem Proving by Deep Graph Embedding** `NeurIPS 2017` [[paper]](https://arxiv.org/abs/1709.09994)

   *Mingzhe Wang, Yihe Tang, Jian Wang, Jia Deng*

5. **Graph2Tac: Online Representation Learning of Formal Math Concepts** `ICML 2024` [[paper]](https://arxiv.org/abs/2401.02949)

   *Lasse Blaauwbroek, Miroslav Olšák, Jason Rute, Fidel Ivan Schaposnik Massolo, Jelle Piepenbrock, Vasily Pestun*
